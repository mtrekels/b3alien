{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76dbf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from b3alien import b3cube, simulation, griis\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import SedonaContext\n",
    "\n",
    "# ---- Environment (as you had it) ----\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/miniconda3\"\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "\n",
    "# ---- Spark/Sedona (once) ----\n",
    "spark_session = (SparkSession.builder\n",
    "    .appName(\"SedonaApp\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.0\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark = SedonaContext.create(spark_session)\n",
    "\n",
    "# ---- Load parquet once ----\n",
    "df = spark.read.format(\"parquet\").load(\"./global_level1_country.parquet\")\n",
    "df = df.withColumn(\"geometry\", f.expr(\"ST_GeomFromWKB(geometry)\"))  # do once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e28a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\"./gbif_downloads_by_country\")\n",
    "RESULTS_CSV = Path(\"./country_simulation_results.csv\")\n",
    "\n",
    "COUNTRY_DIR_RE = re.compile(r\"^(?P<cc>[A-Z]{2})(?:_(?P<n>\\d+))?$\")\n",
    "\n",
    "def list_country_folders(root: Path) -> dict[str, list[Path]]:\n",
    "    \"\"\"\n",
    "    Returns mapping: {\"ES\": [Path(\"ES\"), Path(\"ES_2\"), ...], ...}\n",
    "    Only includes folders that look like country folders.\n",
    "    \"\"\"\n",
    "    grouped: dict[str, list[Path]] = {}\n",
    "    for p in root.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        m = COUNTRY_DIR_RE.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        cc = m.group(\"cc\")\n",
    "        grouped.setdefault(cc, []).append(p)\n",
    "    # stable ordering: base folder first, then numbered\n",
    "    for cc in grouped:\n",
    "        grouped[cc] = sorted(grouped[cc], key=lambda x: (x.name != cc, x.name))\n",
    "    return grouped\n",
    "\n",
    "def find_merged_checklist_file(folder: Path) -> Path | None:\n",
    "    \"\"\"\n",
    "    Accept both spellings: merged_dist.txt and merged_distr.txt\n",
    "    \"\"\"\n",
    "    for name in (\"merged_dist.txt\", \"merged_distr.txt\"):\n",
    "        fp = folder / name\n",
    "        if fp.exists():\n",
    "            return fp\n",
    "    return None\n",
    "\n",
    "def load_merged_specieskeys_for_country(country_folders: list[Path]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Merge all checklists across e.g. ES, ES_2, ES_3 into one unique list of specieskeys.\n",
    "    Skips folders that don't have the merged_dist(r).txt file.\n",
    "    \"\"\"\n",
    "    species_set: set[int] = set()\n",
    "\n",
    "    for folder in country_folders:\n",
    "        fp = find_merged_checklist_file(folder)\n",
    "        if fp is None:\n",
    "            continue\n",
    "        cl = griis.CheckList(str(fp))\n",
    "        # cl.species is expected to be an iterable of ints\n",
    "        species_set.update(int(x) for x in cl.species)\n",
    "\n",
    "    return sorted(species_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab866ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_country_pipeline(country_code: str, specieskeys: list[int]) -> dict:\n",
    "    \"\"\"\n",
    "    Runs your Spark filtering + cumulative curve + Solow-Costello simulation for one country.\n",
    "    Returns a dict of results for CSV.\n",
    "    \"\"\"\n",
    "    if not specieskeys:\n",
    "        return {\n",
    "            \"country_code\": country_code,\n",
    "            \"ok\": False,\n",
    "            \"message\": \"No checklist specieskeys found (missing merged_dist(r).txt?)\",\n",
    "            \"n_species\": 0\n",
    "        }\n",
    "\n",
    "    # 1) Filter parquet by country\n",
    "    df_cc = df.filter(f.col(\"countrycode\") == country_code)\n",
    "\n",
    "    # 2) Filter by checklist species\n",
    "    # Cast to long for safety (you did this too)\n",
    "    species_mask = f.col(\"specieskey\").cast(\"long\").isin([int(x) for x in specieskeys])\n",
    "    df_filtered = df_cc.filter(species_mask)\n",
    "\n",
    "    # 3) First appearance per species\n",
    "    first_seen_df = df_filtered.groupBy(\"specieskey\").agg(\n",
    "        f.min(\"yearmonth\").alias(\"first_appearance\")\n",
    "    )\n",
    "\n",
    "    # 4) New species per month + cumulative\n",
    "    new_species_per_month = (\n",
    "        first_seen_df.groupBy(\"first_appearance\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"new_species_count\")\n",
    "    )\n",
    "\n",
    "    window_spec = Window.orderBy(\"first_appearance\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    df_cumulative = new_species_per_month.withColumn(\n",
    "        \"cumulative_species\",\n",
    "        f.sum(\"new_species_count\").over(window_spec)\n",
    "    )\n",
    "\n",
    "    # 5) To pandas\n",
    "    df_final = df_cumulative.toPandas()\n",
    "    df_final[\"time\"] = pd.to_datetime(df_final[\"first_appearance\"], format=\"%Y-%m\", errors=\"coerce\")\n",
    "    df_final = df_final.sort_values(\"time\").dropna(subset=[\"time\"])\n",
    "\n",
    "    if df_final.empty or df_final[\"cumulative_species\"].max() <= 1:\n",
    "        return {\n",
    "            \"country_code\": country_code,\n",
    "            \"ok\": False,\n",
    "            \"message\": \"Insufficient data after filtering (empty or too few points)\",\n",
    "            \"n_species\": len(specieskeys)\n",
    "        }\n",
    "\n",
    "    # 6) Rate calculation + simulation\n",
    "    time_arr, rate = b3cube.calculate_rate(df_final)\n",
    "\n",
    "    # IMPORTANT: Your original code had `_, vec1 = ...` but later uses `fitted_rate`.\n",
    "    # Most likely the function returns (fitted_rate, params_or_curve). We'll capture that.\n",
    "    fitted_rate, _ = simulation.simulate_solow_costello_scipy(time_arr, rate, vis=False)\n",
    "\n",
    "    results = simulation.parallel_bootstrap_solow_costello(time_arr, rate, n_iterations=500)\n",
    "\n",
    "    if \"beta1_ci\" not in results or \"beta1_samples\" not in results:\n",
    "        raise KeyError(\"Expected keys 'beta1_ci' and 'beta1_samples' not found in bootstrap results.\")\n",
    "\n",
    "    rate_lo, rate_hi = map(float, results[\"beta1_ci\"])\n",
    "    rate_samples = np.asarray(results[\"beta1_samples\"], dtype=float)\n",
    "\n",
    "    return {\n",
    "        \"country_code\": country_code,\n",
    "        \"ok\": True,\n",
    "        \"message\": \"success\",\n",
    "        \"n_species\": len(specieskeys),\n",
    "        \"fitted_rate\": float(fitted_rate),\n",
    "        \"fitted_rate_ci_low\": rate_lo,\n",
    "        \"fitted_rate_ci_high\": rate_hi,\n",
    "        \"fitted_rate_samples_n\": int(rate_samples.size),\n",
    "        # optional: include mean/std of samples to keep CSV compact\n",
    "        \"fitted_rate_samples_mean\": float(np.mean(rate_samples)) if rate_samples.size else np.nan,\n",
    "        \"fitted_rate_samples_std\": float(np.std(rate_samples, ddof=1)) if rate_samples.size > 1 else np.nan,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe905a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_folders = list_country_folders(ROOT)\n",
    "\n",
    "all_rows = []\n",
    "for cc, folders in sorted(country_folders.items()):\n",
    "    print(f\"\\n=== {cc} ({len(folders)} folder(s)) ===\")\n",
    "\n",
    "    specieskeys = load_merged_specieskeys_for_country(folders)\n",
    "    print(f\"Checklist specieskeys: {len(specieskeys)}\")\n",
    "\n",
    "    try:\n",
    "        row = run_country_pipeline(cc, specieskeys)\n",
    "    except Exception as e:\n",
    "        row = {\n",
    "            \"country_code\": cc,\n",
    "            \"ok\": False,\n",
    "            \"message\": f\"Exception: {e}\",\n",
    "            \"n_species\": len(specieskeys),\n",
    "        }\n",
    "\n",
    "    all_rows.append(row)\n",
    "\n",
    "res_df = pd.DataFrame(all_rows)\n",
    "\n",
    "# Append-safe write: if file exists, append without header; else write with header\n",
    "if RESULTS_CSV.exists():\n",
    "    res_df.to_csv(RESULTS_CSV, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    res_df.to_csv(RESULTS_CSV, index=False)\n",
    "\n",
    "res_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
